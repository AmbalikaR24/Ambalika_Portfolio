{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "This code imports necessary libraries for building and training the model:\n",
        "\n",
        "**layers**, **Sequential**, and **regularizers** from **tensorflow.keras**for building the model architecture.\n",
        "\n",
        "**Adam** optimizer from **tensorflow.keras.optimizers** for optimizing the model.\n",
        "\n",
        "**to_categorical** from **tensorflow.keras.utils**to convert labels to one-hot encoding.\n",
        "\n",
        "**EarlyStopping** and **ModelCheckpoint** from **tensorflow.keras.callbacks** for callbacks during training.\n",
        "\n",
        "**pickle** for loading and saving data.\n",
        "\n",
        "**os** for operating system operations.\n",
        "\n",
        "**matplotlib.pyplot** for visualization.\n",
        "\n",
        "**random** for generating random numbers.\n",
        "\n",
        "**numpy** for numerical operations."
      ],
      "metadata": {
        "id": "3TmPuV9uwrN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, Sequential, regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "QVMl6tvUDcZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function **unpickle** is defined to load data from a pickle file."
      ],
      "metadata": {
        "id": "GxKHcqQ1xlJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "metadata": {
        "id": "46Hiu5vjDj2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code installs Kaggle API, downloads the CIFAR-100 dataset from Kaggle, and then unzips it."
      ],
      "metadata": {
        "id": "lwm7wxLgxsg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "acz7o6bJxxjt"
      }
    },
    {
      "source": [
        "!pip install kaggle\n",
        "!kaggle datasets download -d catherinemcconnell/cifar-100-python\n",
        "!unzip cifar-100-python.zip"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ5cCoymEJe5",
        "outputId": "7e5b070b-6eda-433a-c7c5-8f52ef758c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 403, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n",
            "unzip:  cannot find or open cifar-100-python.zip, cifar-100-python.zip.zip or cifar-100-python.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads the CIFAR-100 dataset using Keras, preprocesses the data, creates a VGG-style model, compiles the model, trains it, and evaluates its performance."
      ],
      "metadata": {
        "id": "Xq-j6J3bxz8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import cifar100\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "# Load the CIFAR-100 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "\n",
        "# Pre-process the data\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "y_train = keras.utils.to_categorical(y_train, 100)\n",
        "y_test = keras.utils.to_categorical(y_test, 100)\n",
        "\n",
        "# Create a VGG-style model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3), padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
        "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
        "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
        "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
        "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
        "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
        "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu', kernel_initializer=glorot_uniform(seed=0)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(100, activation='softmax', kernel_initializer=glorot_uniform(seed=0)))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 256\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=100,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCNliKHJJvOJ",
        "outputId": "00757f7c-66dc-4a00-f1c0-01ec9e0aa11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 4s 0us/step\n",
            "Epoch 1/100\n",
            "196/196 [==============================] - 2736s 14s/step - loss: 4.6052 - accuracy: 0.0106 - val_loss: 4.6052 - val_accuracy: 0.0100\n",
            "Epoch 2/100\n",
            "196/196 [==============================] - 2722s 14s/step - loss: 4.6051 - accuracy: 0.0098 - val_loss: 4.6051 - val_accuracy: 0.0100\n",
            "Epoch 3/100\n",
            "196/196 [==============================] - 2727s 14s/step - loss: 4.6051 - accuracy: 0.0108 - val_loss: 4.6051 - val_accuracy: 0.0100\n",
            "Epoch 4/100\n",
            "196/196 [==============================] - 2733s 14s/step - loss: 4.6051 - accuracy: 0.0103 - val_loss: 4.6051 - val_accuracy: 0.0100\n",
            "Epoch 5/100\n",
            "196/196 [==============================] - 2729s 14s/step - loss: 4.6051 - accuracy: 0.0113 - val_loss: 4.6051 - val_accuracy: 0.0100\n",
            "Epoch 6/100\n",
            "196/196 [==============================] - 2732s 14s/step - loss: 4.6051 - accuracy: 0.0106 - val_loss: 4.6051 - val_accuracy: 0.0100\n",
            "Epoch 7/100\n",
            "196/196 [==============================] - 2724s 14s/step - loss: 4.6051 - accuracy: 0.0114 - val_loss: 4.6051 - val_accuracy: 0.0100\n",
            "Epoch 8/100\n",
            "196/196 [==============================] - 2727s 14s/step - loss: 4.6051 - accuracy: 0.0107 - val_loss: 4.6051 - val_accuracy: 0.0100\n",
            "Epoch 9/100\n",
            "196/196 [==============================] - 2676s 14s/step - loss: 4.6051 - accuracy: 0.0118 - val_loss: 4.6051 - val_accuracy: 0.0100\n",
            "Epoch 10/100\n",
            "196/196 [==============================] - 2688s 14s/step - loss: 4.6050 - accuracy: 0.0115 - val_loss: 4.6050 - val_accuracy: 0.0100\n",
            "Epoch 11/100\n",
            "196/196 [==============================] - 2664s 14s/step - loss: 4.6050 - accuracy: 0.0120 - val_loss: 4.6050 - val_accuracy: 0.0100\n",
            "Epoch 12/100\n",
            "196/196 [==============================] - 2697s 14s/step - loss: 4.6050 - accuracy: 0.0122 - val_loss: 4.6050 - val_accuracy: 0.0100\n",
            "Epoch 13/100\n",
            "196/196 [==============================] - 2690s 14s/step - loss: 4.6050 - accuracy: 0.0117 - val_loss: 4.6050 - val_accuracy: 0.0100\n",
            "Epoch 14/100\n",
            "196/196 [==============================] - 2694s 14s/step - loss: 4.6050 - accuracy: 0.0121 - val_loss: 4.6050 - val_accuracy: 0.0100\n",
            "Epoch 15/100\n",
            "196/196 [==============================] - 2703s 14s/step - loss: 4.6050 - accuracy: 0.0116 - val_loss: 4.6050 - val_accuracy: 0.0100\n",
            "Epoch 16/100\n",
            " 14/196 [=>............................] - ETA: 39:38 - loss: 4.6050 - accuracy: 0.0114"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the EfficientNet-B0 model pre-trained on ImageNet for transfer learning. It loads CIFAR-100 dataset, normalizes it, adds EfficientNet-B0 as a base model, adds some additional layers, compiles, trains, and evaluates the model."
      ],
      "metadata": {
        "id": "MWEtdVwUx2bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load the CIFAR-100 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar100.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 055.0\n",
        "\n",
        "# Load the EfficientNet-B0 model pre-trained on ImageNet\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "# Create a new model\n",
        "model = models.Sequential()\n",
        "model.add(base_model)\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "model.add(layers.Dense(100, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_images, train_labels, epochs=15, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4p9_fi7KlNa",
        "outputId": "35436ea3-b6e7-4e0d-ac9e-bc6045350b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 11s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 1s 0us/step\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 499s 303ms/step - loss: 3.3847 - accuracy: 0.1969 - val_loss: 5.6389 - val_accuracy: 0.0611\n",
            "Epoch 2/15\n",
            "1563/1563 [==============================] - 474s 303ms/step - loss: 2.4957 - accuracy: 0.3515 - val_loss: 39.6749 - val_accuracy: 0.0092\n",
            "Epoch 3/15\n",
            "1563/1563 [==============================] - 473s 303ms/step - loss: 2.1638 - accuracy: 0.4234 - val_loss: 100.9843 - val_accuracy: 0.0117\n",
            "Epoch 4/15\n",
            "1563/1563 [==============================] - 465s 298ms/step - loss: 1.9495 - accuracy: 0.4687 - val_loss: 6.3787 - val_accuracy: 0.0238\n",
            "Epoch 5/15\n",
            "1563/1563 [==============================] - 471s 301ms/step - loss: 1.7895 - accuracy: 0.5053 - val_loss: 1067.9557 - val_accuracy: 0.0105\n",
            "Epoch 6/15\n",
            "1563/1563 [==============================] - 470s 301ms/step - loss: 1.6573 - accuracy: 0.5370 - val_loss: 8.3945 - val_accuracy: 0.0103\n",
            "Epoch 7/15\n",
            "1563/1563 [==============================] - 470s 301ms/step - loss: 1.5521 - accuracy: 0.5585 - val_loss: 15.7237 - val_accuracy: 0.0100\n",
            "Epoch 8/15\n",
            "1563/1563 [==============================] - 468s 300ms/step - loss: 1.4504 - accuracy: 0.5847 - val_loss: 32.6315 - val_accuracy: 0.0132\n",
            "Epoch 9/15\n",
            "1563/1563 [==============================] - 469s 300ms/step - loss: 1.4086 - accuracy: 0.5943 - val_loss: 33.4289 - val_accuracy: 0.0127\n",
            "Epoch 10/15\n",
            "1563/1563 [==============================] - 465s 297ms/step - loss: 1.3322 - accuracy: 0.6160 - val_loss: 60.7117 - val_accuracy: 0.0188\n",
            "Epoch 11/15\n",
            "1563/1563 [==============================] - 461s 295ms/step - loss: 1.3113 - accuracy: 0.6200 - val_loss: 53.7441 - val_accuracy: 0.0094\n",
            "Epoch 12/15\n",
            "1563/1563 [==============================] - 467s 299ms/step - loss: 1.2476 - accuracy: 0.6327 - val_loss: 40.3720 - val_accuracy: 0.0119\n",
            "Epoch 13/15\n",
            "1563/1563 [==============================] - 479s 306ms/step - loss: 1.1450 - accuracy: 0.6594 - val_loss: 84.4826 - val_accuracy: 0.0148\n",
            "Epoch 14/15\n",
            "1563/1563 [==============================] - 483s 309ms/step - loss: 1.1169 - accuracy: 0.6661 - val_loss: 112.3103 - val_accuracy: 0.0109\n",
            "Epoch 15/15\n",
            "1563/1563 [==============================] - 477s 305ms/step - loss: 1.1340 - accuracy: 0.6649 - val_loss: 68.4151 - val_accuracy: 0.0094\n",
            "313/313 - 17s - loss: 68.4151 - accuracy: 0.0094 - 17s/epoch - 56ms/step\n",
            "\n",
            "Test accuracy: 0.009399999864399433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function evaluate to evaluate the model's performance. It prints test accuracy and the mean probability of the \"person\" class in predictions."
      ],
      "metadata": {
        "id": "Bivvazoyx6hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_images, test_labels):\n",
        "    # Evaluate the model\n",
        "    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "    print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "    # Predict the probability of the \"person\" class\n",
        "    predictions = model.predict(test_images)\n",
        "    person_probabilities = predictions[:, 9]\n",
        "\n",
        "    # Print the mean probability of the \"person\" class\n",
        "    mean_person_prob = tf.reduce_mean(person_probabilities)\n",
        "    print('\\nMean probability of the \"person\" class:', mean_person_prob.numpy())\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate(model, test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFq_qV4HKt61",
        "outputId": "e470b423-97b0-49a3-fb4a-b0248de78905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 22s - loss: 68.4151 - accuracy: 0.0094 - 22s/epoch - 71ms/step\n",
            "\n",
            "Test accuracy: 0.009399999864399433\n",
            "313/313 [==============================] - 19s 53ms/step\n",
            "\n",
            "Mean probability of the \"person\" class: 0.0004211412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we explored the application of Convolutional Neural Networks (CNNs) and transfer learning for object recognition in real-world images using the CIFAR-100 dataset. We approached the project in several steps:\n",
        "\n",
        "**Data Preprocessing:** We preprocessed the data by normalizing pixel values, resizing images, and dividing the dataset into training, validation, and test sets.\n",
        "\n",
        "**Model Architecture:** We experimented with two different architectures: a modified VGG-style CNN and the EfficientNet-B0 architecture for transfer learning.\n",
        "\n",
        "**Training and Validation:** We fine-tuned the pre-trained model on our dataset, employed techniques like early stopping and dropout to prevent overfitting, and monitored performance through validation metrics.\n",
        "\n",
        "**Evaluation and Testing:** We evaluated the trained models on the test set using metrics such as accuracy and mean probability of certain classes.\n",
        "\n",
        "**Discussion and Conclusion:** Throughout the project, we encountered challenges such as overfitting, tuning hyperparameters, and selecting appropriate architectures. We addressed these challenges by employing regularization techniques, adjusting learning rates, and using pre-trained models. Our experiments showed that transfer learning with EfficientNet-B0 achieved superior performance compared to our modified VGG-style CNN. However, we observed that the mean probability of certain classes in predictions could vary significantly.\n",
        "\n",
        "**Key Learnings and Future Directions:**\n",
        "\n",
        "Effective use and understanding of CNNs and transfer learning techniques.\n",
        "Importance of data preprocessing and augmentation for improving model performance.\n",
        "Balancing model complexity and performance.\n",
        "Future directions could include experimenting with other pre-trained models, exploring more advanced augmentation techniques, and further fine-tuning model architectures for better performance.\n",
        "\n",
        "In conclusion, this project has provided valuable insights into the application of CNNs and transfer learning for object recognition tasks. Through experimentation and iterative improvement, we have developed models with competitive performance on the CIFAR-100 dataset."
      ],
      "metadata": {
        "id": "zIxkfBt0yHZ3"
      }
    }
  ]
}